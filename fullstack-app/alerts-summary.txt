- alert: AlertmanagerFailedReload
description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
summary: Reloading an Alertmanager configuration has failed.
- alert: AlertmanagerMembersInconsistent
description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.
summary: A member of an Alertmanager cluster has not found all other cluster members.
- alert: AlertmanagerFailedToSendAlerts
description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.
summary: An Alertmanager instance failed to send notifications.
- alert: AlertmanagerClusterFailedToSendAlerts
description: The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
summary: All Alertmanager instances in a cluster failed to send notifications to a critical integration.
- alert: AlertmanagerClusterFailedToSendAlerts
description: The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.
summary: All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.
- alert: AlertmanagerConfigInconsistent
description: Alertmanager instances within the {{$labels.job}} cluster have different configurations.
summary: Alertmanager instances within the same cluster have different configurations.
- alert: AlertmanagerClusterDown
description: '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.'
summary: Half or more of the Alertmanager instances within the same cluster are down.
- alert: AlertmanagerClusterCrashlooping
description: '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.'
summary: Half or more of the Alertmanager instances within the same cluster are crashlooping.
- alert: ConfigReloaderSidecarErrors
description: 'Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
summary: config-reloader sidecar has not had a successful reload for 10m
- alert: TargetDown
description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
summary: One or more targets are unreachable.
- alert: Watchdog
description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
summary: An alert that should always be firing to certify that Alertmanager is working properly.
- alert: InfoInhibitor
description: 'This is an alert that is used to inhibit info alerts.
summary: Info-level alert inhibition.
- alert: KubeAPIErrorBudgetBurn
description: The API server is burning too much error budget.
summary: The API server is burning too much error budget.
- alert: KubeAPIErrorBudgetBurn
description: The API server is burning too much error budget.
summary: The API server is burning too much error budget.
- alert: KubeAPIErrorBudgetBurn
description: The API server is burning too much error budget.
summary: The API server is burning too much error budget.
- alert: KubeAPIErrorBudgetBurn
description: The API server is burning too much error budget.
summary: The API server is burning too much error budget.
- alert: KubeStateMetricsListErrors
description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
summary: kube-state-metrics is experiencing errors in list operations.
- alert: KubeStateMetricsWatchErrors
description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
summary: kube-state-metrics is experiencing errors in watch operations.
- alert: KubeStateMetricsShardingMismatch
description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
summary: kube-state-metrics sharding is misconfigured.
- alert: KubeStateMetricsShardsMissing
description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
summary: kube-state-metrics shards are missing.
- alert: KubePodCrashLooping
description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
summary: Pod is crash looping.
- alert: KubePodNotReady
description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
summary: Pod has been in a non-ready state for more than 15 minutes.
- alert: KubeDeploymentGenerationMismatch
description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
summary: Deployment generation mismatch due to possible roll-back
- alert: KubeDeploymentReplicasMismatch
description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
summary: Deployment has not matched the expected number of replicas.
- alert: KubeDeploymentRolloutStuck
description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.
summary: Deployment rollout is not progressing.
- alert: KubeStatefulSetReplicasMismatch
description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
summary: StatefulSet has not matched the expected number of replicas.
- alert: KubeStatefulSetGenerationMismatch
description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
summary: StatefulSet generation mismatch due to possible roll-back
- alert: KubeStatefulSetUpdateNotRolledOut
description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
summary: StatefulSet update has not been rolled out.
- alert: KubeDaemonSetRolloutStuck
description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
summary: DaemonSet rollout is stuck.
- alert: KubeContainerWaiting
description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
summary: Pod container waiting longer than 1 hour
- alert: KubeDaemonSetNotScheduled
description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
summary: DaemonSet pods are not scheduled.
- alert: KubeDaemonSetMisScheduled
description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
summary: DaemonSet pods are misscheduled.
- alert: KubeJobNotCompleted
description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
summary: Job did not complete in time
- alert: KubeJobFailed
description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
summary: Job failed to complete.
- alert: KubeHpaReplicasMismatch
description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
summary: HPA has not matched desired number of replicas.
- alert: KubeHpaMaxedOut
description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
summary: HPA is running at max replicas
- alert: KubeCPUOvercommit
description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
summary: Cluster has overcommitted CPU resource requests.
- alert: KubeMemoryOvercommit
description: Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.
summary: Cluster has overcommitted memory resource requests.
- alert: KubeCPUQuotaOvercommit
description: Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests for Namespaces.
summary: Cluster has overcommitted CPU resource requests.
- alert: KubeMemoryQuotaOvercommit
description: Cluster {{ $labels.cluster }}  has overcommitted memory resource requests for Namespaces.
summary: Cluster has overcommitted memory resource requests.
- alert: KubeQuotaAlmostFull
description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
summary: Namespace quota is going to be full.
- alert: KubeQuotaFullyUsed
description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
summary: Namespace quota is fully used.
- alert: KubeQuotaExceeded
description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
summary: Namespace quota has exceeded the limits.
- alert: CPUThrottlingHigh
description: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
summary: Processes experience elevated CPU throttling.
- alert: KubePersistentVolumeFillingUp
description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.
summary: PersistentVolume is filling up.
- alert: KubePersistentVolumeFillingUp
description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
summary: PersistentVolume is filling up.
- alert: KubePersistentVolumeInodesFillingUp
description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.
summary: PersistentVolumeInodes are filling up.
- alert: KubePersistentVolumeInodesFillingUp
description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.
summary: PersistentVolumeInodes are filling up.
- alert: KubePersistentVolumeErrors
description: The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.
summary: PersistentVolume is having issues with provisioning.
- alert: KubeClientCertificateExpiration
description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
summary: Client certificate is about to expire.
- alert: KubeClientCertificateExpiration
description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
summary: Client certificate is about to expire.
- alert: KubeAggregatedAPIErrors
description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.
summary: Kubernetes aggregated API has reported errors.
- alert: KubeAggregatedAPIDown
description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.
summary: Kubernetes aggregated API is down.
- alert: KubeAPIDown
description: KubeAPI has disappeared from Prometheus target discovery.
summary: Target disappeared from Prometheus target discovery.
- alert: KubeAPITerminatedRequests
description: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.
- alert: KubeControllerManagerDown
description: KubeControllerManager has disappeared from Prometheus target discovery.
summary: Target disappeared from Prometheus target discovery.
- alert: KubeProxyDown
description: KubeProxy has disappeared from Prometheus target discovery.
summary: Target disappeared from Prometheus target discovery.
- alert: KubeNodeNotReady
description: '{{ $labels.node }} has been unready for more than 15 minutes.'
summary: Node is not ready.
- alert: KubeNodeUnreachable
description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
summary: Node is unreachable.
- alert: KubeletTooManyPods
description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.
summary: Kubelet is running at capacity.
- alert: KubeNodeReadinessFlapping
description: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
summary: Node readiness status is flapping.
- alert: KubeletPlegDurationHigh
description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.
summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
- alert: KubeletPodStartUpLatencyHigh
description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.
summary: Kubelet Pod startup latency is too high.
- alert: KubeletClientCertificateExpiration
description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
summary: Kubelet client certificate is about to expire.
- alert: KubeletClientCertificateExpiration
description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
summary: Kubelet client certificate is about to expire.
- alert: KubeletServerCertificateExpiration
description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
summary: Kubelet server certificate is about to expire.
- alert: KubeletServerCertificateExpiration
description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.
summary: Kubelet server certificate is about to expire.
- alert: KubeletClientCertificateRenewalErrors
description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).
summary: Kubelet has failed to renew its client certificate.
- alert: KubeletServerCertificateRenewalErrors
description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).
summary: Kubelet has failed to renew its server certificate.
- alert: KubeletDown
description: Kubelet has disappeared from Prometheus target discovery.
summary: Target disappeared from Prometheus target discovery.
- alert: KubeSchedulerDown
description: KubeScheduler has disappeared from Prometheus target discovery.
summary: Target disappeared from Prometheus target discovery.
- alert: KubeVersionMismatch
description: There are {{ $value }} different semantic versions of Kubernetes components running.
summary: Different semantic versions of Kubernetes components running.
- alert: KubeClientErrors
description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
summary: Kubernetes API server client is experiencing errors.
- alert: NodeFilesystemSpaceFillingUp
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.
summary: Filesystem is predicted to run out of space within the next 24 hours.
- alert: NodeFilesystemSpaceFillingUp
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.
summary: Filesystem is predicted to run out of space within the next 4 hours.
- alert: NodeFilesystemAlmostOutOfSpace
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
summary: Filesystem has less than 5% space left.
- alert: NodeFilesystemAlmostOutOfSpace
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.
summary: Filesystem has less than 3% space left.
- alert: NodeFilesystemFilesFillingUp
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.
summary: Filesystem is predicted to run out of inodes within the next 24 hours.
- alert: NodeFilesystemFilesFillingUp
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
summary: Filesystem is predicted to run out of inodes within the next 4 hours.
- alert: NodeFilesystemAlmostOutOfFiles
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
summary: Filesystem has less than 5% inodes left.
- alert: NodeFilesystemAlmostOutOfFiles
description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
summary: Filesystem has less than 3% inodes left.
- alert: NodeNetworkReceiveErrs
description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
summary: Network interface is reporting many receive errors.
- alert: NodeNetworkTransmitErrs
description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
summary: Network interface is reporting many transmit errors.
- alert: NodeHighNumberConntrackEntriesUsed
description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
summary: Number of conntrack are getting close to the limit.
- alert: NodeTextFileCollectorScrapeError
description: Node Exporter text file collector on {{ $labels.instance }} failed to scrape.
summary: Node Exporter text file collector failed to scrape.
- alert: NodeClockSkewDetected
description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
summary: Clock skew detected.
- alert: NodeClockNotSynchronising
description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
summary: Clock not synchronising.
- alert: NodeRAIDDegraded
description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
summary: RAID Array is degraded.
- alert: NodeRAIDDiskFailure
description: At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
summary: Failed device in RAID array.
- alert: NodeFileDescriptorLimit
description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
summary: Kernel is predicted to exhaust file descriptors limit soon.
- alert: NodeFileDescriptorLimit
description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
summary: Kernel is predicted to exhaust file descriptors limit soon.
- alert: NodeCPUHighUsage
description: 'CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
summary: High CPU usage.
- alert: NodeSystemSaturation
description: 'System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
summary: System saturated, load per core is very high.
- alert: NodeMemoryMajorPagesFaults
description: 'Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
summary: Memory major page faults are occurring at very high rate.
- alert: NodeMemoryHighUtilization
description: 'Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
summary: Host is running out of memory.
- alert: NodeDiskIOSaturation
description: 'Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f" $value }}.
summary: Disk IO queue is high.
- alert: NodeSystemdServiceFailed
description: Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}
summary: Systemd service has entered failed state.
- alert: NodeBondingDegraded
description: Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.
summary: Bonding interface is degraded
- alert: NodeNetworkInterfaceFlapping
description: Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
summary: Network interface is often changing its status
- alert: PrometheusOperatorListErrors
description: Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
summary: Errors while performing list operations in controller.
- alert: PrometheusOperatorWatchErrors
description: Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
summary: Errors while performing watch operations in controller.
- alert: PrometheusOperatorSyncFailed
description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
summary: Last controller reconciliation failed
- alert: PrometheusOperatorReconcileErrors
description: '{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
summary: Errors while reconciling objects.
- alert: PrometheusOperatorStatusUpdateErrors
description: '{{ $value | humanizePercentage }} of status update operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
summary: Errors while updating objects status.
- alert: PrometheusOperatorNodeLookupErrors
description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
summary: Errors while reconciling Prometheus.
- alert: PrometheusOperatorNotReady
description: Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
summary: Prometheus operator not ready
- alert: PrometheusOperatorRejectedResources
description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
summary: Resources rejected by Prometheus operator
- alert: PrometheusBadConfig
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
summary: Failed Prometheus configuration reload.
- alert: PrometheusSDRefreshFailure
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.
summary: Failed Prometheus SD refresh.
- alert: PrometheusNotificationQueueRunningFull
description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
summary: Prometheus alert notification queue predicted to run full in less than 30m.
- alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
- alert: PrometheusNotConnectedToAlertmanagers
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
summary: Prometheus is not connected to any Alertmanagers.
- alert: PrometheusTSDBReloadsFailing
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.
summary: Prometheus has issues reloading blocks from disk.
- alert: PrometheusTSDBCompactionsFailing
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
summary: Prometheus has issues compacting blocks.
- alert: PrometheusNotIngestingSamples
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
summary: Prometheus is not ingesting samples.
- alert: PrometheusDuplicateTimestamps
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
summary: Prometheus is dropping samples with duplicate timestamps.
- alert: PrometheusOutOfOrderTimestamps
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
summary: Prometheus drops samples with out-of-order timestamps.
- alert: PrometheusRemoteStorageFailures
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
summary: Prometheus fails to send samples to remote storage.
- alert: PrometheusRemoteWriteBehind
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
summary: Prometheus remote write is behind.
- alert: PrometheusRemoteWriteDesiredShards
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="kube-prometheus-stack-prometheus",namespace="monitoring"}` $labels.instance | query | first | value }}.
summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
- alert: PrometheusRuleFailures
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.
summary: Prometheus is failing rule evaluations.
- alert: PrometheusMissingRuleEvaluations
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
- alert: PrometheusTargetLimitHit
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded the configured target_limit.
summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
- alert: PrometheusLabelLimitHit
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
summary: Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
- alert: PrometheusScrapeBodySizeLimitHit
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.
summary: Prometheus has dropped some targets that exceeded body size limit.
- alert: PrometheusScrapeSampleLimitHit
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.
summary: Prometheus has failed scrapes that have exceeded the configured sample limit.
- alert: PrometheusTargetSyncFailure
description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.'
summary: Prometheus has failed to sync targets.
- alert: PrometheusHighQueryLoad
description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.
summary: Prometheus is reaching its maximum capacity serving concurrent requests.
- alert: PrometheusErrorSendingAlertsToAnyAlertmanager
description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
